{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Imports Etc"
      ],
      "metadata": {
        "id": "8Xpqkmtz7Afp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIrVujvVnj83"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate seqeval optuna pandas\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import BertModel, BertTokenizerFast, TrainingArguments, Trainer, DataCollatorForTokenClassification, TrainerCallback, TrainerControl, TrainerState\n",
        "import evaluate\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import optuna\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "pretrained_model_name = 'bert-base-uncased'\n",
        "os.environ[\"WANDB_DISABLED\"] = 'true'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1. Extend Pretrained BERT with a Classifier\n"
      ],
      "metadata": {
        "id": "Lk9Nf8Cf2_2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, bert_model: BertModel, num_labels: int, dropout_rate: Optional[float] = None) -> None:\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.num_labels = num_labels\n",
        "        self.classifier = nn.Linear(bert_model.config.hidden_size, num_labels)\n",
        "        if dropout_rate is not None:\n",
        "            self.dropout = nn.Dropout(dropout_rate)\n",
        "        else:\n",
        "            self.dropout = nn.Dropout(bert_model.config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor,\n",
        "                attention_mask: torch.Tensor,\n",
        "                token_type_ids: torch.Tensor,\n",
        "                labels: Optional[torch.Tensor] = None) -> Union[Tuple[torch.Tensor, Any], torch.Tensor]:\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids)\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "        output = (logits,) + outputs[2:]\n",
        "        return (loss,) + output if loss is not None else output\n",
        "\n"
      ],
      "metadata": {
        "id": "soigMnn13FuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 2. Prepare Finetuning Dataset and Define Freeze Function\n"
      ],
      "metadata": {
        "id": "uQWYlFLl3QxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wnut = load_dataset(\"wnut_17\")\n",
        "label_list = wnut[\"train\"].features[\"ner_tags\"].feature.names\n",
        "print('Available NER tags:', label_list)\n",
        "num_labels = len(label_list)\n",
        "print('Number of NER tags in the dataset:', num_labels)\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name)\n",
        "tokenized_input = tokenizer(wnut['train'][0][\"tokens\"], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print('Tokenized input:', tokenized_input)\n",
        "print('\\nInput:', wnut['train'][0][\"tokens\"])\n",
        "print('Tokenized input ids:', tokenized_input['input_ids'])\n",
        "print('Tokenized input tokens:', tokens)\n",
        "print('\\nLength of Input:', len(wnut['train'][0][\"tokens\"]))\n",
        "print('Length of Tokenized input:', len(tokenized_input['input_ids']))\n",
        "print('Length of Target labels:', len(wnut['train'][0][\"ner_tags\"]))\n",
        "\n",
        "def tokenize_and_align_labels(examples: Dict[str, Any],\n",
        "                              tokenizer: BertTokenizerFast) -> Dict[str, Any]:\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
        "train_dataset_hf, eval_dataset_hf, test_dataset_hf = tokenized_wnut[\"train\"], tokenized_wnut[\"validation\"], tokenized_wnut[\"test\"]\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "def freeze_bert_layers(model: nn.Module, unfreeze_last_n: int) -> nn.Module:\n",
        "    \"\"\"\n",
        "    Freeze all parameters in the model, then unfreeze only the classifier and the last `unfreeze_last_n`\n",
        "    layers of the BERT encoder.\n",
        "\n",
        "    Parameters:\n",
        "      model (nn.Module): The BertClassifier model.\n",
        "      unfreeze_last_n (int): The number of last BERT encoder layers to unfreeze.\n",
        "\n",
        "    Returns:\n",
        "      nn.Module: The model with gradients enabled only for the classifier and the last n encoder layers.\n",
        "    \"\"\"\n",
        "    for name, param in model.named_parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    for name, param in model.classifier.named_parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    total_layers: int = len(model.bert.encoder.layer)\n",
        "    for i in range(total_layers - unfreeze_last_n, total_layers):\n",
        "        for param in model.bert.encoder.layer[i].parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    return model\n",
        "\n",
        "def compute_metrics(model_output):\n",
        "    \"\"\"\n",
        "    Compute evaluation metrics to check the performance of the model during training (on the validation set)\n",
        "    and after training (on the test set).\n",
        "    The input parameter of the function is a Tuple as required by the HuggingFace Trainer.\n",
        "\n",
        "    Parameters:\n",
        "    - model_output (Tuple): Contains model's raw predictions and target labels.\n",
        "\n",
        "    Returns:\n",
        "    - dict: Dictionary of the evaluation metrics, i.e. Precision, Recall, F1, and Accuracy.\n",
        "    \"\"\"\n",
        "    predictions, labels = model_output\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "CFAdidNE3Th1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 3. Model Initialization and Baseline Training\n"
      ],
      "metadata": {
        "id": "owRcIuUI3tz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = BertModel.from_pretrained(pretrained_model_name)\n",
        "num_labels = len(wnut[\"train\"].features[\"ner_tags\"].feature.names)\n",
        "baseline_model = BertClassifier(bert_model, num_labels=num_labels).to(device)\n",
        "\n",
        "baseline_training_args = TrainingArguments(\n",
        "    output_dir='./model_output',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "baseline_trainer = Trainer(\n",
        "    model=baseline_model,\n",
        "    args=baseline_training_args,\n",
        "    train_dataset=train_dataset_hf,\n",
        "    eval_dataset=eval_dataset_hf,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "print(\"Starting baseline training (all layers unfrozen)...\")\n",
        "baseline_trainer.train()\n",
        "print(\"Baseline evaluation on validation set:\")\n",
        "print(baseline_trainer.evaluate())\n",
        "baseline_predictions = baseline_trainer.predict(test_dataset_hf)\n",
        "print(\"Baseline evaluation on test set:\")\n",
        "print(baseline_predictions.metrics)\n"
      ],
      "metadata": {
        "id": "aAOWUilL3pwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Bayesian Hyperparameter Tuning with Optuna\n",
        "\n"
      ],
      "metadata": {
        "id": "H5S90UBE3zLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_results = []\n",
        "\n",
        "for l in range(1, 5):\n",
        "    print(f\"\\nNumber of unfrozen layers: {l}\\n\")\n",
        "\n",
        "    hyper_params = {}\n",
        "\n",
        "    def model_init() -> torch.nn.Module:\n",
        "        bert_model = BertModel.from_pretrained(pretrained_model_name)\n",
        "        dropout_rate = hyper_params.get(\"dropout_rate\", None)\n",
        "        model = BertClassifier(bert_model, num_labels=num_labels, dropout_rate=dropout_rate).to(device)\n",
        "        model = freeze_bert_layers(model, hyper_params.get(\"unfreeze_last_n\", l))\n",
        "        return model\n",
        "\n",
        "    def hp_space(trial: optuna.Trial) -> Dict[str, Any]:\n",
        "        hyper_params[\"unfreeze_last_n\"] = l\n",
        "        learning_rate = trial.suggest_float(\"learning_rate\", 3e-5, 7e-5, log=True)\n",
        "        per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [32])\n",
        "        warmup_steps = trial.suggest_categorical(\"warmup_steps\", [500, 750])\n",
        "        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.3)\n",
        "        hyper_params[\"dropout_rate\"] = dropout_rate\n",
        "        return {\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"per_device_train_batch_size\": per_device_train_batch_size,\n",
        "            \"warmup_steps\": warmup_steps,\n",
        "        }\n",
        "\n",
        "    tuning_args = TrainingArguments(\n",
        "        output_dir='./hp_tuning_output',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=64,\n",
        "        learning_rate=5e-5,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./hp_tuning_logs',\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        save_strategy=\"no\",\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    tuner = Trainer(\n",
        "        model_init=model_init,\n",
        "        args=tuning_args,\n",
        "        train_dataset=train_dataset_hf,\n",
        "        eval_dataset=eval_dataset_hf,\n",
        "        compute_metrics=compute_metrics,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "    print(\"Starting hyperparameter search...\")\n",
        "    best_trial = tuner.hyperparameter_search(\n",
        "        hp_space=hp_space,\n",
        "        direction=\"maximize\",\n",
        "        n_trials=10\n",
        "    )\n",
        "    print(\"Best trial hyperparameters:\", best_trial.hyperparameters)\n",
        "\n",
        "    final_training_args = TrainingArguments(\n",
        "        output_dir='./final_model_output',\n",
        "        num_train_epochs=5,\n",
        "        per_device_train_batch_size=best_trial.hyperparameters[\"per_device_train_batch_size\"],\n",
        "        per_device_eval_batch_size=64,\n",
        "        learning_rate=best_trial.hyperparameters[\"learning_rate\"],\n",
        "        warmup_steps=best_trial.hyperparameters[\"warmup_steps\"],\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./final_logs',\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=100,\n",
        "        load_best_model_at_end=True,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    final_model = model_init()\n",
        "    final_trainer = Trainer(\n",
        "        model=final_model,\n",
        "        args=final_training_args,\n",
        "        train_dataset=train_dataset_hf,\n",
        "        eval_dataset=eval_dataset_hf,\n",
        "        compute_metrics=compute_metrics,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "    print(\"Starting final training with best hyperparameters...\")\n",
        "    final_trainer.train()\n",
        "    val_metrics = final_trainer.evaluate()\n",
        "    print(\"Final evaluation on validation set:\")\n",
        "    print(val_metrics)\n",
        "    test_predictions = final_trainer.predict(test_dataset_hf)\n",
        "    test_metrics = test_predictions.metrics\n",
        "    print(\"Final evaluation on test set:\")\n",
        "    print(test_metrics)\n",
        "\n",
        "    exp_result = {\n",
        "        \"unfreeze_last_n\": l,\n",
        "        \"learning_rate\": best_trial.hyperparameters[\"learning_rate\"],\n",
        "        \"warmup_steps\": best_trial.hyperparameters[\"warmup_steps\"],\n",
        "        \"dropout_rate\": hyper_params.get(\"dropout_rate\"),\n",
        "        \"val_precision\": val_metrics.get(\"precision\"),\n",
        "        \"val_recall\": val_metrics.get(\"recall\"),\n",
        "        \"val_f1\": val_metrics.get(\"f1\"),\n",
        "        \"val_accuracy\": val_metrics.get(\"accuracy\"),\n",
        "        \"test_precision\": test_metrics.get(\"precision\"),\n",
        "        \"test_recall\": test_metrics.get(\"recall\"),\n",
        "        \"test_f1\": test_metrics.get(\"f1\"),\n",
        "        \"test_accuracy\": test_metrics.get(\"accuracy\"),\n",
        "    }\n",
        "    experiment_results.append(exp_result)\n",
        "    print(\"\\n-------------\\n\")\n",
        "\n",
        "df_results = pd.DataFrame(experiment_results)\n",
        "df_results.to_csv(\"experiment_results.csv\", index=False)\n",
        "print(\"Saved experiment results to 'experiment_results.csv'\")\n"
      ],
      "metadata": {
        "id": "kJDUPZvw30mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DL for NLP Lab 7 Report\n",
        "**Author**: Oisín Redmond\n",
        "\n",
        "## Methodology\n",
        "\n",
        "It is crucial for future machine learning models to be more computationally and energy efficient if the massive amount of CO₂ produced by AI is to be addressed. Parameter-efficient fine-tuning is a way for models to train for specific tasks on a small number of their original parameters, saving both time and energy. This lab explored the effect of partial fine-tuning on a pre-trained BERT model for NER.\n",
        "\n",
        "The `freeze_bert_layers` function was created and used to disable gradient updates for all encoder layers except the final *n* layers and the classification head. Models with `unfreeze_last_n` set to 1, 2, 3, and 4 were evaluated. This function uses `param.requires_grad` and `model.named_parameters` to freeze any number of the original 12 BERT layers.\n",
        "\n",
        "Hyperparameters (learning rate, batch size, warmup steps, and dropout) were optimised using Optuna, an open-source hyperparameter optimisation framework designed to automate the search for optimal hyperparameter configurations in machine learning models. The performance of each configuration was assessed on the test set using F1 score, precision, recall, and accuracy.\n",
        "\n",
        "## Discussion\n",
        "\n",
        "Results show that selectively unfreezing layers can yield strong performance with reduced computational cost. As more layers were unfrozen, test F1 improved consistently. The trade-off between number of layers and performance is highlighted below.\n",
        "\n",
        "|   | Unfrozen Layers | Test F1 | Test Precision | Test Recall | Test Accuracy |\n",
        "|---|------------------|---------|----------------|-------------|----------------|\n",
        "| 0 | Baseline (all)   | 0.4007  | 0.5192         | 0.3262      | 0.9400         |\n",
        "| 1 | 1                | 0.3738  | 0.5413         | 0.2854      | 0.9407         |\n",
        "| 2 | 2                | 0.4163  | 0.5365         | 0.3401      | 0.9431         |\n",
        "| 3 | 3                | 0.4250  | 0.5492         | 0.3466      | 0.9438         |\n",
        "| 4 | 4                | 0.4676  | 0.5658         | 0.3985      | 0.9466         |\n",
        "\n",
        "**Table:** Test set performance across configurations.\n",
        "\n",
        "These findings suggest that with effective hyperparameter tuning, freezing most of the BERT encoder still allows strong task adaptation. Notably, the optimiser often retained default values for warmup steps and batch size, suggesting further efficiency gains. Overall, these experiments indicated partial fine-tuning can outperform naïve full fine-tuning, offering a better performance–compute trade-off.\n"
      ],
      "metadata": {
        "id": "F1KJztTI4WwT"
      }
    }
  ]
}