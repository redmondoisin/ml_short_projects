{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Imports etc"
      ],
      "metadata": {
        "id": "nrLVE22Ylibq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets tokenizers\n",
        "!pip install torch --upgrade\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from contextlib import nullcontext\n",
        "from google.colab import drive\n",
        "\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "ZF5M5zYt5MNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Tokenizer and Dataset Preperation"
      ],
      "metadata": {
        "id": "oQS5Pbqj-hiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens = [\"[PAD]\", \"[UNK]\", \"[SEP]\", \"[MASK]\"]\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer = BpeTrainer(vocab_size=10000, min_frequency=5, special_tokens=special_tokens)\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "wiki_train_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split='train')\n",
        "tokenizer.train_from_iterator(wiki_train_dataset['text'], trainer=trainer)\n",
        "tokenizer.save(\"./wiki-text-bpe.tokenizer.json\")\n",
        "tokenizer = Tokenizer.from_file(\"./wiki-text-bpe.tokenizer.json\")\n",
        "print(f\"Vocab Size: {tokenizer.get_vocab_size()}\")\n",
        "\n",
        "class MaskedLanguageModelingDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer: Tokenizer, seq_length: int, mask_prob: float = 0.15) -> None:\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_length = seq_length\n",
        "        self.mask_prob = mask_prob\n",
        "        self.pad_token_id = self.tokenizer.token_to_id(\"[PAD]\")\n",
        "        self.mask_token_id = self.tokenizer.token_to_id(\"[MASK]\")\n",
        "        self.vocab_size = self.tokenizer.get_vocab_size()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> tuple:\n",
        "        encoded_text = self.tokenizer.encode(self.texts[idx]).ids\n",
        "        if len(encoded_text) < self.seq_length:\n",
        "            encoded_text += [self.pad_token_id] * (self.seq_length - len(encoded_text))\n",
        "        else:\n",
        "            encoded_text = encoded_text[:self.seq_length]\n",
        "        input_ids = encoded_text.copy()\n",
        "        labels = [-100] * self.seq_length\n",
        "        for i in range(self.seq_length):\n",
        "            if random.random() < self.mask_prob:\n",
        "                labels[i] = input_ids[i]\n",
        "                rand_val = random.random()\n",
        "                if rand_val < 0.8:\n",
        "                    input_ids[i] = self.mask_token_id\n",
        "                elif rand_val < 0.9:\n",
        "                    input_ids[i] = random.randint(0, self.vocab_size - 1)\n",
        "                    assert 0 <= input_ids[i] < self.vocab_size\n",
        "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "seq_length = 512\n",
        "train_dataset = MaskedLanguageModelingDataset(wiki_train_dataset['text'], tokenizer, seq_length)\n",
        "wiki_valid_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split='validation')\n",
        "wiki_test_dataset  = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split='test')\n",
        "valid_dataset = MaskedLanguageModelingDataset(wiki_valid_dataset['text'], tokenizer, seq_length)\n",
        "test_dataset  = MaskedLanguageModelingDataset(wiki_test_dataset['text'], tokenizer, seq_length)"
      ],
      "metadata": {
        "id": "eKRWQHO_-s2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model Definition"
      ],
      "metadata": {
        "id": "2RFpc-db-xak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size: int, heads: int) -> None:\n",
        "        super().__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "        self.values = nn.Linear(embed_size, embed_size)\n",
        "        self.keys = nn.Linear(embed_size, embed_size)\n",
        "        self.queries = nn.Linear(embed_size, embed_size)\n",
        "        self.proj_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        batch_size, seq_len, embed_size = x.size()\n",
        "        all_queries = self.queries(x).view(batch_size, seq_len, self.heads, self.head_dim).transpose(1, 2)\n",
        "        all_keys = self.keys(x).view(batch_size, seq_len, self.heads, self.head_dim).transpose(1, 2)\n",
        "        all_values = self.values(x).view(batch_size, seq_len, self.heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = (all_queries @ all_keys.transpose(-1, -2)) * (1 / math.sqrt(self.head_dim))\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        out = (attn_weights @ all_values).transpose(1, 2).contiguous().view(batch_size, seq_len, embed_size)\n",
        "        return self.proj_out(out)\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, embed_size: int, ff_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_size, ff_size)\n",
        "        self.fc2 = nn.Linear(ff_size, embed_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.fc2(torch.relu(self.fc1(x)))\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_size: int, heads: int, ff_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.norm_attn = nn.LayerNorm(embed_size)\n",
        "        self.norm_ff = nn.LayerNorm(embed_size)\n",
        "        self.self_attention = MultiHeadSelfAttention(embed_size, heads)\n",
        "        self.feed_forward = PositionwiseFeedForward(embed_size, ff_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.self_attention(self.norm_attn(x), mask)\n",
        "        x = x + self.feed_forward(self.norm_ff(x))\n",
        "        return x\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, max_seq_len: int, embed_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(max_seq_len, embed_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        positions = torch.arange(x.size(1), device=x.device).expand(x.size(0), -1)\n",
        "        return x + self.embedding(positions)\n",
        "\n",
        "class BERTModel(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embed_size: int, ff_size: int, num_layers: int, num_heads: int,\n",
        "                 max_seq_len: int = 512, pad_token_id: int = 0) -> None:  #\n",
        "        super().__init__()\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_encoding = PositionalEmbedding(max_seq_len, embed_size)\n",
        "        self.layers = nn.ModuleList([TransformerEncoderLayer(embed_size, num_heads, ff_size) for _ in range(num_layers)])\n",
        "        self.un_embedding = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
        "        mask = (input_ids != self.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
        "        x = self.embedding(input_ids)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.un_embedding(x)\n"
      ],
      "metadata": {
        "id": "aaVbjrrA-0AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Training and Evaluation"
      ],
      "metadata": {
        "id": "XiIkWPeq-5Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_from_drive = input(\"Load model from Google Drive? (y/n): \").strip().lower()\n",
        "drive_mounted = False\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "drive_mounted = os.path.exists('/content/drive/MyDrive')\n",
        "\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "embed_size = 768\n",
        "ff_size = 3072\n",
        "num_layers = 12\n",
        "num_heads = 12\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "if load_from_drive == \"y\":\n",
        "    model = BERTModel(vocab_size, embed_size, ff_size, num_layers, num_heads, seq_length).to(device)\n",
        "    model_path = input(\"Enter the full path to your model file in Google Drive: \").strip()\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        print(\"Model loaded successfully from Google Drive.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the model: {e}\")\n",
        "    further_training = (input(\"Would you like to further train the model? (y/n): \").strip().lower() == \"y\")\n",
        "else:\n",
        "    further_training = True\n",
        "    model = BERTModel(\n",
        "    vocab_size=tokenizer.get_vocab_size(),\n",
        "    embed_size=768,\n",
        "    ff_size=3072,\n",
        "    num_layers=12,\n",
        "    num_heads=12,\n",
        "    pad_token_id=tokenizer.token_to_id(\"[PAD]\")\n",
        "    ).to(device)\n",
        "    def init_weights(m: nn.Module) -> None:\n",
        "        for _, param in m.named_parameters():\n",
        "            nn.init.normal_(param.data, mean=0.0, std=0.02)\n",
        "    model.apply(init_weights)\n",
        "\n",
        "if further_training:\n",
        "    num_epochs = int(input(\"Enter the number of epochs to train: \").strip())\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
        "    pad_token_id = tokenizer.token_to_id(\"[PAD]\")\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "    num_workers = 4 if torch.cuda.is_available() else 0\n",
        "    pin_memory = True if torch.cuda.is_available() else False\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        autocast = lambda: torch.amp.autocast(device_type='cuda')\n",
        "        scaler = torch.amp.GradScaler()\n",
        "    else:\n",
        "        autocast = nullcontext\n",
        "        scaler = None\n",
        "\n",
        "    def train(model: nn.Module, dataset: Dataset, optimizer: optim.Optimizer, criterion: nn.Module, clip: float) -> float:\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
        "        for input_ids, labels in tqdm(dataloader, total=len(dataloader), desc='Training'):\n",
        "            input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                output = model(input_ids)\n",
        "                loss = criterion(output.view(-1, output.shape[-1]), labels.view(-1))\n",
        "            if scaler is not None:\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "                optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        return epoch_loss / len(dataloader)\n",
        "\n",
        "    def evaluate(model: nn.Module, dataset: Dataset, criterion: nn.Module) -> float:\n",
        "        model.eval()\n",
        "        epoch_loss = 0.0\n",
        "        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
        "        with torch.no_grad():\n",
        "            for input_ids, labels in tqdm(dataloader, total=len(dataloader), desc='Eval'):\n",
        "                input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "                with autocast():\n",
        "                    output = model(input_ids)\n",
        "                    loss = criterion(output.view(-1, output.shape[-1]), labels.view(-1))\n",
        "                epoch_loss += loss.item()\n",
        "        return epoch_loss / len(dataloader)\n",
        "\n",
        "    def epoch_time(start_time: float, end_time: float) -> tuple:\n",
        "        elapsed = end_time - start_time\n",
        "        return int(elapsed / 60), int(elapsed % 60)\n",
        "\n",
        "    best_valid_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        train_loss = train(model, train_dataset, optimizer, criterion, clip=1.0)\n",
        "        valid_loss = evaluate(model, valid_dataset, criterion)\n",
        "        end_time = time.time()\n",
        "        mins, secs = epoch_time(start_time, end_time)\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), 'BERT-LM-model.pt')\n",
        "        scheduler.step(valid_loss)\n",
        "        print(f\"Epoch {epoch+1} | Time: {mins}m {secs}s | LR: {optimizer.param_groups[0]['lr']:.1e}\")\n",
        "        print(f\"Train Loss: {train_loss:.3f}, PPL: {math.exp(train_loss):.3f}\")\n",
        "        print(f\"Val Loss:   {valid_loss:.3f}, PPL: {math.exp(valid_loss):.3f}\")\n",
        "        if (epoch + 1) % 1 == 0 and drive_mounted:\n",
        "            if input(\"Save to Drive? (y/n): \").strip().lower() == \"y\":\n",
        "                torch.save(model.state_dict(), '/content/drive/MyDrive/dl-for_nlp-week-6-model.pt')\n"
      ],
      "metadata": {
        "id": "pf8k9R-m-9yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Inference and Testing"
      ],
      "metadata": {
        "id": "L3ub3IlF_B2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_masked_tokens(model: nn.Module, tokenizer: Tokenizer, sentence: str, temperature: float = 0.7, top_k: int = 50) -> str:\n",
        "    model.eval()\n",
        "    modified = sentence.replace(\"<mask>\", \"[MASK]\")\n",
        "    encoded = tokenizer.encode(modified).ids\n",
        "    input_ids = torch.tensor([encoded], dtype=torch.long).to(device)\n",
        "    with torch.no_grad():\n",
        "        with (torch.amp.autocast(device_type='cuda') if torch.cuda.is_available() else nullcontext()):\n",
        "            output = model(input_ids)\n",
        "    predicted = []\n",
        "    mask_token_id = tokenizer.token_to_id(\"[MASK]\")\n",
        "    for idx, token_id in enumerate(input_ids[0]):\n",
        "        if token_id == mask_token_id:\n",
        "            logits = output[0, idx] / temperature\n",
        "            topk_logits, topk_ids = torch.topk(logits, top_k)\n",
        "            probs = torch.softmax(topk_logits, dim=-1)\n",
        "            pred_id = topk_ids[torch.multinomial(probs, 1).item()].item()\n",
        "            decoded = tokenizer.decode([pred_id]).strip().replace(\"##\", \"\")\n",
        "            predicted.append(decoded)\n",
        "    for token in predicted:\n",
        "        modified = modified.replace(\"[MASK]\", token, 1)\n",
        "    return modified\n",
        "\n",
        "def evaluate_validation_masked_tokens(model: nn.Module, tokenizer: Tokenizer, validation_texts: list, num_samples: int = 20) -> float:\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    samples = random.sample(validation_texts, min(num_samples, len(validation_texts)))\n",
        "    for text in samples:\n",
        "        words = text.split()\n",
        "        if len(words) < 2:\n",
        "            continue\n",
        "        idx = random.randint(0, len(words) - 1)\n",
        "        original = words[idx]\n",
        "        words[idx] = \"<mask>\"\n",
        "        masked = \" \".join(words)\n",
        "        prediction = predict_masked_tokens(model, tokenizer, masked)\n",
        "        pred_word = prediction.split()[idx] if idx < len(prediction.split()) else \"\"\n",
        "        print(f\"Original: {original}, Predicted: {pred_word}\")\n",
        "        if pred_word.strip().lower() == original.strip().lower():\n",
        "            correct += 1\n",
        "        count += 1\n",
        "    acc = correct / count if count > 0 else 0.0\n",
        "    print(f\"Validation Accuracy: {acc:.3f}\")\n",
        "    return acc\n",
        "\n",
        "_ = evaluate_validation_masked_tokens(model, tokenizer, wiki_valid_dataset['text'])\n",
        "\n",
        "if input(\"Save final model to Drive? (y/n): \").strip().lower() == \"y\" and drive_mounted:\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/dl-for_nlp-week-6-model.pt')\n",
        "    print(\"Final model saved.\")\n"
      ],
      "metadata": {
        "id": "ZeRAV8ft_E0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DL for NLP Lab 6 Report\n",
        "\n",
        "## Methodology\n",
        "\n",
        "The `MultiHeadSelfAttention` class was adapted from a GPT-style causal attention mechanism to a BERT-style bidirectional attention mechanism. This modification involved removing causal masking to enable tokens to attend to both preceding and succeeding tokens within sequences. This bidirectional attention is essential for masked language modelling tasks typically encountered during BERT pretraining.\n",
        "\n",
        "The `MaskedLanguageModelingDataset` class constructs training samples suitable for masked language modelling, a critical component of BERT pretraining. The dataset implementation applies random masking to tokens, following an 80-10-10 masking strategy: 80% of masked tokens are replaced by the `[MASK]` token, 10% by random tokens, and 10% remain unchanged. This stochastic masking method enhances the robustness of the model. Labels are initialised with a value of `-100`, allowing the loss function to ignore unmasked positions and consequently reducing computational overhead. Furthermore, the complete Wikitext dataset was loaded in an attempt to enhance model performance.\n",
        "\n",
        "Several optimisations have been implemented to accelerate model training and inference. These include adopting mixed-precision training via PyTorch's automatic mixed precision (AMP), which leverages Tensor Cores available on Colab GPUs to enhance computational efficiency while reducing memory requirements. The training procedure incorporates gradient scaling through `torch.amp.GradScaler`, ensuring stable gradient updates even with reduced precision. Additionally, gradient clipping using `torch.nn.utils.clip_grad_norm_` further stabilises training by mitigating issues related to exploding gradients without adversely affecting model performance.\n",
        "\n",
        "Moreover, `DataLoader` configurations using parallel data loading (with `num_workers=4`) and pin memory (`pin_memory=True`) maximise GPU utilisation by efficiently handling data transfer between CPU and GPU memory. The AdamW optimiser, combined with a learning rate scheduler (`ReduceLROnPlateau`), dynamically adjusts learning rates based on validation performance, further optimising training efficiency.\n",
        "\n",
        "## Discussion\n",
        "\n",
        "The results demonstrate the BERT-style model's ability to learn from the masked language modelling task. However, the results weren't very promising no matter how much data and complexity was added to the model. Perplexity appeared to stabilise around `2.4` on both the training and validation sets, even when trained on an A100 GPU using the entire dataset. It is suspected that there might be an issue with the masking process, despite considerable efforts spent refining it. Another possibility is that previous weaker models might still be used during evaluation due to potential leakage from Google Drive. Overall, this task felt close to completion, yet the model never fully achieved optimal results due to constraints related to time and computational resources.\n",
        "\n"
      ],
      "metadata": {
        "id": "-l8FFINq03s7"
      }
    }
  ]
}