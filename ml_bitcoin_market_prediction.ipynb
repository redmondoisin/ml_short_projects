{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM2IgqBdxoptpntwF2gKNX3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install all dependencies\n",
        "!pip install -q yfinance statsmodels xgboost optuna nltk seaborn requests autogluon.tabular\n",
        "!python -m nltk.downloader -q vader_lexicon punkt stopwords\n"
      ],
      "metadata": {
        "id": "r2GfZNQEfdrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports & Utility Functions\n",
        "import os, re, requests\n",
        "from datetime import datetime\n",
        "import numpy as np, pandas as pd, yfinance as yf\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    mean_absolute_error, mean_squared_error,\n",
        "    accuracy_score, f1_score, roc_auc_score\n",
        ")\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from xgboost import XGBRegressor\n",
        "from autogluon.tabular import TabularPredictor\n",
        "import optuna\n",
        "import torch, torch.nn as nn\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n"
      ],
      "metadata": {
        "id": "mvS6HmTDb5De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Data‐Collection Functions & Caching\n",
        "def load_or_fetch_csv(fname, fetch_func, parse_dates=None, force_fetch=False, **kw):\n",
        "    if os.path.exists(fname) and not force_fetch:\n",
        "        return pd.read_csv(fname, parse_dates=parse_dates)\n",
        "    df = fetch_func(**kw)\n",
        "    if not df.empty:\n",
        "        df.to_csv(fname, index=False)\n",
        "    return df\n",
        "\n",
        "def get_cc_data(fsym=\"BTC\", tsym=\"USD\", limit=2000, api_key=None):\n",
        "    url=\"https://min-api.cryptocompare.com/data/v2/histoday\"\n",
        "    params={\"fsym\":fsym,\"tsym\":tsym,\"limit\":limit}\n",
        "    hdr={}\n",
        "    if api_key: hdr[\"authorization\"]=f\"Apikey {api_key}\"\n",
        "    r=requests.get(url,params=params,headers=hdr)\n",
        "    js=r.json().get(\"Data\",{}).get(\"Data\",[])\n",
        "    df=pd.DataFrame(js)\n",
        "    df['date']=pd.to_datetime(df.time,unit='s')\n",
        "    df.rename(columns={'close':'price','volumeto':'volume'},inplace=True)\n",
        "    return df[['date','price','volume']]\n",
        "\n",
        "def get_cp_news(api_key, currency=\"BTC\"):\n",
        "    url=\"https://cryptopanic.com/api/v1/posts/\"\n",
        "    params={\"auth_token\":api_key,\"public\":\"true\",\"currencies\":currency}\n",
        "    r=requests.get(url,params=params)\n",
        "    items=r.json().get(\"results\",[])\n",
        "    out=[]\n",
        "    for i in items:\n",
        "        out.append({\n",
        "            'date':pd.to_datetime(i['published_at']).date(),\n",
        "            'title':i['title']\n",
        "        })\n",
        "    return pd.DataFrame(out)\n",
        "\n",
        "def get_guardian(api_key, max_pages=5):\n",
        "    items=[]\n",
        "    for page in range(1, max_pages+1):\n",
        "        params={\"api-key\":api_key,\"section\":\"us-news\",\"page\":page,\"page-size\":50,\"show-fields\":\"headline\"}\n",
        "        r=requests.get(\"https://content.guardianapis.com/search\",params=params)\n",
        "        res=r.json().get(\"response\",{}).get(\"results\",[])\n",
        "        if not res: break\n",
        "        for it in res:\n",
        "            date=it.get(\"webPublicationDate\",\"\")[:10]\n",
        "            items.append({'date':pd.to_datetime(date), 'title':it['fields']['headline']})\n",
        "    return pd.DataFrame(items)\n",
        "\n",
        "def get_world_bank(country=\"USA\", indicator=\"FP.CPI.TOTL.ZG\", date_range=\"2010:2024\"):\n",
        "    url=f\"http://api.worldbank.org/v2/country/{country}/indicator/{indicator}\"\n",
        "    params={\"format\":\"json\",\"date\":date_range,\"per_page\":200}\n",
        "    r=requests.get(url,params=params)\n",
        "    data=r.json()\n",
        "    if len(data)>1:\n",
        "        df=pd.DataFrame(data[1])\n",
        "        df['date']=pd.to_datetime(df['date'],format='%Y')\n",
        "        return df[['date','value']]\n",
        "    return pd.DataFrame()\n",
        "\n",
        "# API keys from Colab userdata\n",
        "from google.colab import userdata\n",
        "CC_KEY = userdata.get('CC_KEY'); CP_KEY=userdata.get('CP_KEY'); GN_KEY=userdata.get('GN_KEY')\n",
        "\n",
        "# Fetch & cache\n",
        "market_df      = load_or_fetch_csv(\"market.csv\", get_cc_data, parse_dates=[\"date\"], api_key=CC_KEY)\n",
        "cp_df          = load_or_fetch_csv(\"cryptopanic.csv\", get_cp_news,               api_key=CP_KEY, force_fetch=True)\n",
        "guardian_df    = load_or_fetch_csv(\"guardian.csv\",   get_guardian,   parse_dates=[\"date\"], api_key=GN_KEY)\n",
        "inflation_df   = load_or_fetch_csv(\"inflation.csv\",  get_world_bank)\n",
        "\n",
        "print(\"Market:\", market_df.shape, \"CryptoPanic:\", cp_df.shape,\n",
        "      \"Guardian:\", guardian_df.shape, \"Inflation:\", inflation_df.shape)\n"
      ],
      "metadata": {
        "id": "rIdqDpxKfga_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Preprocessing & Sentiment\n",
        "def preprocess_market(df):\n",
        "    df=df.sort_values(\"date\").set_index(\"date\").asfreq(\"D\").ffill().reset_index()\n",
        "    return df\n",
        "\n",
        "def preprocess_macro(df):\n",
        "    df=df.dropna().sort_values(\"date\").set_index(\"date\").asfreq(\"YS\").ffill().reset_index()\n",
        "    return df\n",
        "\n",
        "market_clean   = preprocess_market(market_df)\n",
        "inflation_clean= preprocess_macro(inflation_df)\n",
        "\n",
        "sia=SentimentIntensityAnalyzer()\n",
        "cp_df['sentiment']      = cp_df['title'].apply(lambda t: sia.polarity_scores(t)['compound'])\n",
        "guardian_df['sentiment']= guardian_df['title'].apply(lambda t: sia.polarity_scores(t)['compound'])\n",
        "\n",
        "print(\"Cleaned & Sentimentized\")\n"
      ],
      "metadata": {
        "id": "EnD0KaAVfh8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Exploratory Analysis\n",
        "# 5.1 EMA & Seasonality\n",
        "market_clean['EMA90']=market_clean['price'].ewm(span=90).mean()\n",
        "fig,ax=plt.subplots(1,2,figsize=(12,4))\n",
        "ax[0].plot(market_clean.date,market_clean.price,label='Price')\n",
        "ax[0].plot(market_clean.date,market_clean.EMA90,label='EMA90'); ax[0].legend(); ax[0].set_title(\"Price vs EMA\")\n",
        "res=seasonal_decompose(market_clean.set_index(\"date\")['price'], model='additive', period=365)\n",
        "res.plot(); plt.suptitle(\"Seasonality\"); plt.tight_layout()\n",
        "\n",
        "# 5.2 RSI & Volatility\n",
        "delta=market_clean.price.diff()\n",
        "gain=delta.clip(lower=0); loss=-delta.clip(upper=0)\n",
        "avg_gain=gain.rolling(14).mean(); avg_loss=loss.rolling(14).mean()\n",
        "rs=avg_gain/avg_loss; market_clean['RSI']=100-(100/(1+rs))\n",
        "market_clean['vol7']=delta.rolling(7).std()\n",
        "plt.figure(figsize=(6,3)); plt.plot(market_clean.date,market_clean.RSI); plt.title(\"RSI\")\n",
        "\n",
        "# 5.3 News vs Price\n",
        "daily_guardian=guardian_df.groupby('date')['sentiment'].mean().reset_index()\n",
        "mrg=pd.merge(market_clean, daily_guardian, on='date', how='inner')\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.plot(mrg.date, mrg.price, label='Price')\n",
        "plt.plot(mrg.date, mrg.sentiment, label='Sentiment')\n",
        "plt.legend(); plt.title(\"Price vs Guardian Sentiment\")\n"
      ],
      "metadata": {
        "id": "-CCtNhetfjmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Feature Engineering for Modeling\n",
        "# combine signals into one DF\n",
        "df = market_clean[['date','price','volume','EMA90','RSI','vol7']].copy()\n",
        "df = df.rename(columns={'price':'Close'})\n",
        "# merge latest daily sentiment & on-chain stubs\n",
        "sent = guardian_df.groupby('date')['sentiment'].mean()\n",
        "df['Sentiment'] = df['date'].map(sent).fillna(0)\n",
        "# stub blockchain metrics\n",
        "np.random.seed(0)\n",
        "df['TxCount'] = np.random.randint(10000,30000,len(df))\n",
        "df['ActiveAddresses'] = np.random.randint(5000,20000,len(df))\n",
        "# targets & lags\n",
        "df['Return']=df.Close.pct_change()\n",
        "df['Lag1']=df.Return.shift(1)\n",
        "df['Lag7']=df.Return.shift(7)\n",
        "df['Volatility7']=df.Return.rolling(7).std()\n",
        "df['Direction']=(df.Return>0).astype(int)\n",
        "df['VolNext']=df.Volatility7.shift(-1)\n",
        "df.dropna(inplace=True)\n",
        "df = df.set_index('date')\n",
        "features = ['Close','volume','EMA90','RSI','vol7','Sentiment','TxCount','ActiveAddresses','Lag1','Lag7','Volatility7']\n"
      ],
      "metadata": {
        "id": "rK2T5FURflQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Auto‑ARIMA via Optuna\n",
        "ts = df['Close']\n",
        "ts = ts.asfreq('D')\n",
        "def arima_obj(trial):\n",
        "    p=trial.suggest_int(\"p\",0,5); d=trial.suggest_int(\"d\",0,2); q=trial.suggest_int(\"q\",0,5)\n",
        "    try:\n",
        "        m=ARIMA(ts,order=(p,d,q),trend='t').fit()\n",
        "        return m.aic\n",
        "    except:\n",
        "        return 1e10\n",
        "\n",
        "study=optuna.create_study(direction='minimize'); study.optimize(arima_obj,n_trials=25)\n",
        "best=study.best_params; order=(best['p'],best['d'],best['q'])\n",
        "print(\"Best order:\",order)\n",
        "model_arima=ARIMA(ts,order=order,trend='t').fit()\n",
        "fc=model_arima.get_forecast(30)\n",
        "pred_arima, ci = fc.predicted_mean, fc.conf_int()\n",
        "arima_mae = mean_absolute_error(df.Close[-30:], pred_arima)\n",
        "arima_rmse= mean_squared_error(df.Close[-30:], pred_arima, squared=False)\n"
      ],
      "metadata": {
        "id": "muZJCE6SfnPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: XGBoost w/ Rolling CV + Optuna HPO\n",
        "# prepare X,y\n",
        "X = df[features].values; y=df['Close'].shift(-1).dropna().values\n",
        "X=X[:-1]; # align\n",
        "tscv=TimeSeriesSplit(n_splits=4)\n",
        "\n",
        "# Optuna tuning\n",
        "def xgb_obj(tr):\n",
        "    params={\n",
        "      'n_estimators': tr.suggest_int('n_estimators',50,300),\n",
        "      'max_depth': tr.suggest_int('max_depth',3,12),\n",
        "      'learning_rate':tr.suggest_loguniform('learning_rate',1e-3,1e-1),\n",
        "      'subsample':tr.suggest_uniform('subsample',0.6,1),\n",
        "      'colsample_bytree':tr.suggest_uniform('colsample_bytree',0.6,1),\n",
        "      'tree_method':\"gpu_hist\" if DEVICE.type=='cuda' else 'hist'\n",
        "    }\n",
        "    rmses=[]\n",
        "    for tr_idx,val_idx in tscv.split(X):\n",
        "        m=XGBRegressor(**params).fit(X[tr_idx],y[tr_idx])\n",
        "        p=m.predict(X[val_idx]); rmses.append(mean_squared_error(y[val_idx],p,squared=False))\n",
        "    return np.mean(rmses)\n",
        "\n",
        "st2=optuna.create_study(direction='minimize'); st2.optimize(xgb_obj, n_trials=20)\n",
        "xgb_params=st2.best_params; xgb_params['tree_method']=\"gpu_hist\" if DEVICE.type=='cuda' else 'hist'\n",
        "# final eval\n",
        "split=int(0.8*len(X))\n",
        "xgb_final=XGBRegressor(**xgb_params).fit(X[:split],y[:split])\n",
        "pred_xgb=xgb_final.predict(X[split:])\n",
        "xgb_opt_mae=mean_absolute_error(y[split:],pred_xgb)\n",
        "xgb_opt_rmse=mean_squared_error(y[split:],pred_xgb,squared=False)\n"
      ],
      "metadata": {
        "id": "Xvq9_alUfozB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 (improved): LSTM with target‑scaling\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "from torch.cuda.amp        import autocast, GradScaler\n",
        "from torch.utils.data      import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics       import mean_absolute_error, mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# —— 1) Scale features + target ——\n",
        "feat_scaler = StandardScaler()\n",
        "X_all = feat_scaler.fit_transform(df[features].values)\n",
        "\n",
        "tgt_scaler = StandardScaler()\n",
        "y_all = df[\"Close\"].shift(-1).dropna().values.reshape(-1,1)\n",
        "y_scaled = tgt_scaler.fit_transform(y_all).flatten()\n",
        "\n",
        "# align lengths\n",
        "X_all = X_all[:-1]  # drop last row so X & y match\n",
        "\n",
        "# —— 2) Create sequences ——\n",
        "def mk_seq(arr: np.ndarray, tgt: np.ndarray, L: int = 20):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(arr) - L):\n",
        "        Xs.append(arr[i : i + L])\n",
        "        ys.append(tgt[i + L])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "Xseq, Yseq = mk_seq(X_all, y_scaled, L=20)\n",
        "\n",
        "# —— 3) Train/test split ——\n",
        "n = len(Xseq)\n",
        "s = int(0.8 * n)\n",
        "Xtr, Xte = Xseq[:s], Xseq[s:]\n",
        "ytr, yte = Yseq[:s], Yseq[s:]\n",
        "\n",
        "# to tensors\n",
        "Xt = torch.tensor(Xtr, dtype=torch.float32).to(DEVICE)\n",
        "yt = torch.tensor(ytr, dtype=torch.float32).view(-1,1).to(DEVICE)\n",
        "Xv = torch.tensor(Xte, dtype=torch.float32).to(DEVICE)\n",
        "yv = torch.tensor(yte, dtype=torch.float32).view(-1,1).to(DEVICE)\n",
        "\n",
        "# —— 4) Define model ——\n",
        "class LSTMReg(nn.Module):\n",
        "    def __init__(self, n_feats: int, hidden: int = 64):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(n_feats, hidden, batch_first=True)\n",
        "        self.fc   = nn.Linear(hidden, 1)\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        _, (h, _) = self.lstm(x)\n",
        "        return self.fc(h[-1])\n",
        "\n",
        "lstm = LSTMReg(Xtr.shape[2]).to(DEVICE)\n",
        "opt   = torch.optim.Adam(lstm.parameters(), lr=1e-3)\n",
        "scaler_amp = GradScaler()\n",
        "\n",
        "# —— 5) Train ——\n",
        "loader = DataLoader(TensorDataset(Xt, yt), batch_size=64, shuffle=False)\n",
        "for ep in range(15):\n",
        "    lstm.train()\n",
        "    total_loss = 0.0\n",
        "    for xb, yb in loader:\n",
        "        opt.zero_grad()\n",
        "        with autocast():\n",
        "            out   = lstm(xb)\n",
        "            loss  = nn.MSELoss()(out, yb)\n",
        "        scaler_amp.scale(loss).backward()\n",
        "        scaler_amp.step(opt)\n",
        "        scaler_amp.update()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "    print(f\"Epoch {ep+1:02d}, train MSE loss = {total_loss/len(loader.dataset):.4f}\")\n",
        "\n",
        "# —— 6) Predict & inverse‑scale ——\n",
        "lstm.eval()\n",
        "with torch.no_grad():\n",
        "    pred_scaled = lstm(Xv).cpu().numpy().flatten()\n",
        "\n",
        "# back to USD\n",
        "pred_lstm = tgt_scaler.inverse_transform(pred_scaled.reshape(-1,1)).flatten()\n",
        "actual_lstm = tgt_scaler.inverse_transform(yte.reshape(-1,1)).flatten()\n",
        "\n",
        "# —— 7) Metrics ——\n",
        "lstm_mae  = mean_absolute_error(actual_lstm, pred_lstm)\n",
        "lstm_rmse = mean_squared_error(actual_lstm, pred_lstm, squared=False)\n",
        "print(f\"LSTM Price → MAE: {lstm_mae:.2f}, RMSE: {lstm_rmse:.2f}\")\n",
        "\n",
        "# direction metrics (aligned)\n",
        "n2 = min(len(actual_lstm), len(pred_lstm))\n",
        "dp = (pred_lstm[:n2][1:] > pred_lstm[:n2][:-1]).astype(int)\n",
        "da = (actual_lstm[:n2][1:] > actual_lstm[:n2][:-1]).astype(int)\n",
        "print(\"LSTM Direction Acc:\", accuracy_score(da, dp))\n",
        "print(\"LSTM Direction F1: \",    f1_score(da, dp))\n",
        "print(\"LSTM Direction ROC:\",    roc_auc_score(da, dp))\n"
      ],
      "metadata": {
        "id": "71W1UAtXlUND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10 (fast‐train + GPU): AutoGluon Baseline\n",
        "from autogluon.tabular import TabularPredictor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, f1_score\n",
        "\n",
        "# avoid duplicating \"Close\"\n",
        "ag_feats = [f for f in features if f != \"Close\"]\n",
        "\n",
        "# assemble DataFrame\n",
        "ag_df = df[ag_feats + [\"Close\", \"Direction\", \"VolNext\"]].copy()\n",
        "ag_df[\"PriceT\"] = ag_df[\"Close\"].shift(-1)\n",
        "ag_df[\"DirT\"]   = ag_df[\"Direction\"].shift(-1)\n",
        "ag_df[\"VolT\"]   = ag_df[\"VolNext\"]\n",
        "ag_df.dropna(inplace=True)\n",
        "ag_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# train/test split\n",
        "cut = int(0.8 * len(ag_df))\n",
        "train_ag, test_ag = ag_df.iloc[:cut], ag_df.iloc[cut:]\n",
        "\n",
        "# common .fit kwargs for speed + GPU\n",
        "fit_kwargs = dict(\n",
        "    time_limit=120,                   # max seconds\n",
        "    presets='medium_quality_faster_train',  # faster preset\n",
        "    ag_args_fit={'num_gpus': 1}       # force GPU use\n",
        ")\n",
        "\n",
        "# 1) Price regression\n",
        "predictor_price = TabularPredictor(\n",
        "    label=\"PriceT\", problem_type=\"regression\",\n",
        "    eval_metric=\"mean_absolute_error\"\n",
        ").fit(\n",
        "    train_data=train_ag[ag_feats + [\"PriceT\"]],\n",
        "    **fit_kwargs\n",
        ")\n",
        "pred_price_ag = predictor_price.predict(test_ag[ag_feats])\n",
        "ag_price_mae  = mean_absolute_error(test_ag[\"PriceT\"], pred_price_ag)\n",
        "ag_price_rmse = mean_squared_error(test_ag[\"PriceT\"], pred_price_ag, squared=False)\n",
        "print(f\"AutoGluon Price → MAE: {ag_price_mae:.2f}, RMSE: {ag_price_rmse:.2f}\")\n",
        "\n",
        "# 2) Direction classification\n",
        "predictor_dir = TabularPredictor(\n",
        "    label=\"DirT\", problem_type=\"binary\",\n",
        "    eval_metric=\"accuracy\"\n",
        ").fit(\n",
        "    train_data=train_ag[ag_feats + [\"DirT\"]],\n",
        "    **fit_kwargs\n",
        ")\n",
        "pred_dir_ag = predictor_dir.predict(test_ag[ag_feats])\n",
        "ag_dir_acc  = accuracy_score(test_ag[\"DirT\"], pred_dir_ag)\n",
        "ag_dir_f1   = f1_score(test_ag[\"DirT\"], pred_dir_ag)\n",
        "print(f\"AutoGluon Direction → Acc: {ag_dir_acc:.2f}, F1: {ag_dir_f1:.2f}\")\n",
        "\n",
        "# 3) Volatility regression\n",
        "predictor_vol = TabularPredictor(\n",
        "    label=\"VolT\", problem_type=\"regression\",\n",
        "    eval_metric=\"mean_absolute_error\"\n",
        ").fit(\n",
        "    train_data=train_ag[ag_feats + [\"VolT\"]],\n",
        "    **fit_kwargs\n",
        ")\n",
        "pred_vol_ag = predictor_vol.predict(test_ag[ag_feats])\n",
        "ag_vol_mae  = mean_absolute_error(test_ag[\"VolT\"], pred_vol_ag)\n",
        "ag_vol_rmse = mean_squared_error(test_ag[\"VolT\"], pred_vol_ag, squared=False)\n",
        "print(f\"AutoGluon Volatility → MAE: {ag_vol_mae:.4f}, RMSE: {ag_vol_rmse:.4f}\")\n"
      ],
      "metadata": {
        "id": "OEDWxmeDkglV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11 (fixed): Weighted Ensemble + AutoGluon Stacking (with proper alignment)\n",
        "from autogluon.tabular import TabularPredictor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# — 1) Weighted average of XGB + LSTM —\n",
        "\n",
        "# weights inversely proportional to RMSE\n",
        "w1, w2 = 1 / xgb_opt_rmse, 1 / lstm_rmse\n",
        "\n",
        "# true values for XGB come from y[split:], preds from pred_xgb\n",
        "y_true_xgb = y[split:]\n",
        "\n",
        "# find minimum length across the three series\n",
        "n_ens = min(len(y_true_xgb), len(pred_xgb), len(pred_lstm))\n",
        "\n",
        "# crop them all\n",
        "y_true_crop  = y_true_xgb[:n_ens]\n",
        "px           = pred_xgb[:n_ens]\n",
        "pl           = pred_lstm[:n_ens]\n",
        "\n",
        "# compute weighted ensemble\n",
        "pred_ens     = (w1 * px + w2 * pl) / (w1 + w2)\n",
        "ens_mae      = mean_absolute_error(y_true_crop, pred_ens)\n",
        "ens_rmse     = mean_squared_error(y_true_crop, pred_ens, squared=False)\n",
        "\n",
        "print(f\"Weighted Ensemble → MAE: {ens_mae:.2f}, RMSE: {ens_rmse:.2f}\")\n"
      ],
      "metadata": {
        "id": "C0BS5IbWnMSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# — 2) AutoGluon 1‑level stacking —\n",
        "\n",
        "stack_predictor = TabularPredictor(\n",
        "    label=\"PriceT\", problem_type=\"regression\"\n",
        ").fit(\n",
        "    train_data=train_ag[ag_feats + [\"PriceT\"]],\n",
        "    presets=\"best_quality\",\n",
        "    time_limit=120,\n",
        "    ag_args_fit={\n",
        "        \"num_gpus\": 1,\n",
        "        \"stack_ensemble_levels\": 1\n",
        "    }\n",
        ")\n",
        "\n",
        "stack_pred = stack_predictor.predict(test_ag[ag_feats])\n",
        "stack_mae  = mean_absolute_error(test_ag[\"PriceT\"], stack_pred)\n",
        "stack_rmse = mean_squared_error(test_ag[\"PriceT\"], stack_pred, squared=False)\n",
        "\n",
        "print(f\"AutoGluon Stacked → MAE: {stack_mae:.2f}, RMSE: {stack_rmse:.2f}\")\n"
      ],
      "metadata": {
        "id": "_419QmTcs9Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Compare All Models\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rows = [\n",
        "  [\"ARIMA\",          arima_mae,  arima_rmse,   np.nan,       np.nan],\n",
        "  [\"XGBoost(opt)\",   xgb_opt_mae,xgb_opt_rmse, np.nan,       np.nan],\n",
        "  [\"LSTM\",           lstm_mae,   lstm_rmse,    lstm_dir_acc, lstm_dir_f1],\n",
        "  [\"Ensemble(X+L)\",  ens_mae,    ens_rmse,     np.nan,       np.nan],\n",
        "  [\"AutoGluonPrice\", ag_price_mae,ag_price_rmse,np.nan,      np.nan],\n",
        "  [\"AutoGluonDir\",   np.nan,     np.nan,       ag_dir_acc,   ag_dir_f1],\n",
        "  [\"AutoGluonVol\",   ag_vol_mae, ag_vol_rmse,  np.nan,       np.nan],\n",
        "  [\"AGStack\",        stack_mae,  stack_rmse,   np.nan,       np.nan]\n",
        "]\n",
        "df_res = pd.DataFrame(rows, columns=[\"Model\",\"MAE\",\"RMSE\",\"DirAcc\",\"DirF1\"])\n",
        "print(df_res.to_markdown(index=False))\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.barh(df_res.Model, df_res.MAE)\n",
        "plt.xlabel(\"MAE\"); plt.title(\"Model MAE Comparison\"); plt.show()\n"
      ],
      "metadata": {
        "id": "FVEnYzkinN9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Visualize Predictions vs Ground Truth for Each Model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) ARIMA (last 30 days)\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.plot(df.index[-30:], df[\"Close\"].iloc[-30:], label=\"Actual\")\n",
        "plt.plot(pred_arima.index,       pred_arima,        linestyle=\"--\", label=\"ARIMA\")\n",
        "plt.title(\"ARIMA Forecast vs Actual (Last 30 Days)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2) XGBoost (test split)\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.plot(y[split:],        label=\"Actual\")\n",
        "plt.plot(pred_xgb,         linestyle=\"--\", label=\"XGBoost\")\n",
        "plt.title(\"XGBoost Predictions vs Actual\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3) LSTM (test set)\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.plot(yte_arr,          label=\"Actual\")\n",
        "plt.plot(pred_lstm,        linestyle=\"--\", label=\"LSTM\")\n",
        "plt.title(\"LSTM Predictions vs Actual\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4) AutoGluon Price (test_ag)\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.plot(test_ag[\"PriceT\"].values, label=\"Actual\")\n",
        "plt.plot(pred_price_ag,            linestyle=\"--\", label=\"AutoGluon Price\")\n",
        "plt.title(\"AutoGluon Price Predictions vs Actual\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5) Weighted Ensemble (XGB + LSTM)\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.plot(y[split:],     label=\"Actual\")\n",
        "plt.plot(pred_ens,      linestyle=\"--\", label=\"Weighted Ensemble\")\n",
        "plt.title(\"Weighted Ensemble Predictions vs Actual\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6) AutoGluon Stacked (Price)\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.plot(test_ag[\"PriceT\"].values, label=\"Actual\")\n",
        "plt.plot(stack_pred,               linestyle=\"--\", label=\"AutoGluon Stacked\")\n",
        "plt.title(\"AutoGluon Stacked Predictions vs Actual\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VU10jdFHqFZu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}